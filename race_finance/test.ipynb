{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "根据以下问题和选项给出正确的答案。\n",
      "问题：国际支付与结算工具的优缺点是什么？\n",
      "选项：A:国际支付工具方便快捷，可以实现实时资金转移。\n",
      "B:国际支付工具会收取手续费和汇率转换费用。\n",
      "C:国际支付工具在所有国家和地区都可以使用。\n",
      "D:国际支付工具使用先进的加密技术，保证交易安全。\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "What are some commonly used performance metrics to evaluate the success of algorithmic trading strategies in the hedge fund space?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "根据以下问题和选项给出正确的答案。\n",
      "问题：最近几年当地住宅房价的变动趋势如何？\n",
      "选项：A:近年来当地住宅房价持续上涨。\n",
      "B:最近几年当地住宅房价持平，基本没有变动。\n",
      "C:当地住宅房价近年来波动较大，有时上涨有时下跌。\n",
      "D:近年来当地住宅房价持续下跌。\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "根据以下问题和选项给出正确的答案。\n",
      "问题：如何监控企业贷款的风险状况？\n",
      "选项：A:通过定期或不定期的财务报告，对企业的财务状况进行监控，一旦出现风险信号，立即采取措施。\n",
      "B:对企业的信用状况进行详细的评估，包括企业的还款能力、经营状况、财务状况等。\n",
      "C:根据企业的风险状况，设定一定的风险储备金，以应对可能出现的风险。\n",
      "D:通过培训和教育，提高企业对贷款风险的认识，引导企业合理使用贷款。\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "根据以下问题和选项给出正确的答案。\n",
      "问题：如何评估一个健康险产品的竞争力？\n",
      "选项：A:保障范围是评估一个健康险产品竞争力的重要方面。\n",
      "B:保费高低直接影响健康险产品的竞争力。\n",
      "C:理赔服务的好坏对健康险产品的竞争力有很大影响。\n",
      "D:附加服务的质量会影响健康险产品的竞争力。\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "市场监管官员在金融产品监管中的角色是什么？<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "根据以下问题和选项给出正确的答案。\n",
      "问题：How can legal counsel assist financial institutions in navigating and ensuring compliance with banking regulations when developing and implementing a machine learning model for detecting violations?\n",
      "选项：A:Legal counsel can assist financial institutions in interpreting and applying relevant banking laws and regulations to the design and operation of the machine learning model.\n",
      "B:Legal counsel can assist financial institutions in managing financial risks associated with the use of machine learning models.\n",
      "C:Legal counsel can assist financial institutions in marketing and promoting machine learning models to potential clients.\n",
      "D:Legal counsel can assist financial institutions in managing cybersecurity risks associated with the use of machine learning models.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "根据以下问题和选项给出正确的答案。\n",
      "问题：How can we evaluate the effectiveness of social assistance programs in reducing poverty and income inequality?\n",
      "选项：A:Using statistical analysis to measure the impact of the program on poverty and income inequality.\n",
      "B:Comparing regions or populations where the program has been implemented with those where it hasn't.\n",
      "C:Conducting surveys and interviews with the beneficiaries of the program.\n",
      "D:Using economic models to predict the impact of the program and comparing it with the actual results.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "在进行创业投资时，你如何确定一个项目的估值？<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "When conducting financial market analysis, how do you assess the risk associated with different asset classes?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "根据以下问题和选项给出正确的答案。\n",
      "问题：What are the common algorithms used to optimize options trading strategies?\n",
      "选项：A:The Black-Scholes Model is a mathematical model used to calculate the theoretical price of an option.\n",
      "B:The Binomial Tree Model allows for the possibility of early exercise in calculating the price of an American option.\n",
      "C:Monte Carlo Simulation is used to model the probability of different outcomes in a process with random variables.\n",
      "D:Bollinger Bands is a statistical chart used to characterize the prices and volatility of a financial instrument over time.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "税务合规检查中常见的问题有哪些？<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "根据以下问题和选项给出正确的答案。\n",
      "问题：What data sources can an investment analyst use to gather information on fiscal policy and its impact on the economy?\n",
      "选项：A:Government websites, financial news and publications, and academic journals are the primary sources for gathering information on fiscal policy and its impact on the economy.\n",
      "B:Data aggregation platforms, public company filings, and economic research institutions are the primary sources for gathering information on fiscal policy and its impact on the economy.\n",
      "C:Financial news and publications, academic journals, and government websites are the primary sources for gathering information on fiscal policy and its impact on the economy.\n",
      "D:Public company filings, data aggregation platforms, and economic research institutions are the primary sources for gathering information on fiscal policy and its impact on the economy.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "利率政策调控对经济稳定和发展有什么影响？<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "互联网金融监管的主要挑战是什么？<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。<|im_end|>\n",
      "<|im_start|>user\n",
      "根据以下问题和选项给出正确的答案。\n",
      "问题：What are the key indicators used to assess the fiscal sustainability of public finance systems in emerging economies?\n",
      "选项：A:The Public Debt-to-GDP Ratio and the Fiscal Balance are two key indicators used to assess fiscal sustainability.\n",
      "B:The Current Account Balance and the Inflation Rate are two key indicators used to assess fiscal sustainability.\n",
      "C:Tax Revenue and Government Expenditure are two key indicators used to assess fiscal sustainability.\n",
      "D:Sovereign Credit Ratings and the structure and maturity of debt are two key indicators used to assess fiscal sustainability.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m query_types \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch_items]\n\u001b[1;32m     59\u001b[0m unique_ids \u001b[38;5;241m=\u001b[39m [generate_unique_code(prompt) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m---> 61\u001b[0m responses, run_infos \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m responses \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(inp_list, unique_id_list, max_new_tokens)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend(input_id)\n\u001b[0;32m---> 27\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     28\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_new_tokens,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.95\u001b[39m,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     33\u001b[0m }\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, ()\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3021\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3020\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3021\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3023\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3109\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3104\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3105\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3106\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3107\u001b[0m         )\n\u001b[1;32m   3108\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 3109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3111\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3127\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   3132\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   3133\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3151\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3152\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3311\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3301\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3302\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3303\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3304\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3308\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3309\u001b[0m )\n\u001b[0;32m-> 3311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3313\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3329\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3330\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:562\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# (we say ~ because the number of overflow varies with the example in the batch)\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# To match each overflowing sample with the original sample in the batch\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# we add an overflow_to_sample_mapping array (see below)\u001b[39;00m\n\u001b[1;32m    561\u001b[0m sanitized_tokens \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 562\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokens_and_encodings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     stack \u001b[38;5;241m=\u001b[39m [e \u001b[38;5;28;01mfor\u001b[39;00m item, _ \u001b[38;5;129;01min\u001b[39;00m tokens_and_encodings \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m item[key]]\n\u001b[1;32m    564\u001b[0m     sanitized_tokens[key] \u001b[38;5;241m=\u001b[39m stack\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import traceback\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "log_file = \"run_log_qwen25.jsonl\"  # 日志\n",
    "ans_file = \"model_logits_qwen25.jsonl\"  # 推理结果\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,padding_side=\"left\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device, torch_dtype=\"auto\")\n",
    "\n",
    "def generate_response(inp_list, unique_id_list, max_new_tokens=2048):\n",
    "   \n",
    "    # prompts = [f\"User: {inp}\\nAssistant: \" for inp in inp_list]\n",
    "    input_ids = []\n",
    "    for inp in inp_list:\n",
    "        \n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。\"},\n",
    "        {\"role\": \"user\", \"content\": inp}\n",
    "            ]\n",
    "        input_id = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        print(input_id)\n",
    "        continue\n",
    "        input_ids.append(input_id)\n",
    "    inputs = tokenizer(input_ids, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    params = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"top_p\": 0.95,\n",
    "        \"temperature\": 0.8,\n",
    "        \"do_sample\": True\n",
    "    }\n",
    "\n",
    "    return None, ()\n",
    "\n",
    "def generate_unique_code(input_string):\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    sha256_hash.update(input_string.encode('utf-8'))\n",
    "    unique_code = sha256_hash.hexdigest()\n",
    "    return unique_code\n",
    "\n",
    "def load_test_data(test_file):\n",
    "    test_data = []\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data.append(json.loads(line.strip()))\n",
    "    return test_data\n",
    "\n",
    "# 加载测试数据\n",
    "test_data = load_test_data('eval_only_query.jsonl')\n",
    "print(\"test\")\n",
    "\n",
    "batch_size = 16\n",
    "for i in range(0, len(test_data), batch_size):\n",
    "    batch_items = test_data[i:i+batch_size]\n",
    "    prompts = [item['query'] for item in batch_items]\n",
    "    query_types = [item['query_type'] for item in batch_items]\n",
    "    unique_ids = [generate_unique_code(prompt) for prompt in prompts]\n",
    "\n",
    "    responses, run_infos = generate_response(prompts, unique_ids)\n",
    "\n",
    "    if responses is None:\n",
    "        continue\n",
    "\n",
    "    for j in range(len(prompts)):\n",
    "        response = responses[j]\n",
    "        run_info = run_infos[j]\n",
    "        run_info[\"answer\"] = response\n",
    "        # 保存运行日志\n",
    "        with open(log_file, \"a\") as fw:\n",
    "            fw.write(json.dumps(run_info, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # 保存输出结果\n",
    "        answer_info = {\n",
    "            \"query\": prompts[j],\n",
    "            \"query_type\": query_types[j],\n",
    "            \"answer\": response\n",
    "        }\n",
    "\n",
    "        with open(ans_file, \"a\") as fw:\n",
    "            fw.write(json.dumps(answer_info, ensure_ascii=False) + \"\\n\")\n",
    "        print(f\"answer_info: {answer_info}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c5a10d67c949e7b3e68216b30dd332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af62cf7adcb4b3b91d8915c60a78ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "LocalTokenNotFoundError",
     "evalue": "Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLocalTokenNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 36\u001b[0m\n\u001b[1;32m     27\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUGGINGFACEHUB_API_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_ryfKhzuVeRLMrxHewUkbefCcjqJgkDHezG\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# llm = HuggingFaceEndpoint(\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     repo_id=model_name,\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     task=\"text-generation\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     huggingfacehub_api_token=\"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\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatHuggingFace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_response\u001b[39m(inp_list, unique_id_list, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/langchain_huggingface/chat_models/huggingface.py:169\u001b[0m, in \u001b[0;36mChatHuggingFace.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_model_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    172\u001b[0m     AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_id)\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\n\u001b[1;32m    175\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/langchain_huggingface/chat_models/huggingface.py:295\u001b[0m, in \u001b[0;36mChatHuggingFace._resolve_model_id\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resolve the model_id from the LLM's inference_server_url\"\"\"\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_inference_endpoints  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m available_endpoints \u001b[38;5;241m=\u001b[39m \u001b[43mlist_inference_endpoints\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_huggingface_hub(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mrepo_id\n\u001b[1;32m    298\u001b[0m ):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mrepo_id\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/huggingface_hub/hf_api.py:7691\u001b[0m, in \u001b[0;36mHfApi.list_inference_endpoints\u001b[0;34m(self, namespace, token)\u001b[0m\n\u001b[1;32m   7689\u001b[0m \u001b[38;5;66;03m# Special case: list all endpoints for all namespaces the user has access to\u001b[39;00m\n\u001b[1;32m   7690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m namespace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7691\u001b[0m     user \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhoami\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7693\u001b[0m     \u001b[38;5;66;03m# List personal endpoints first\u001b[39;00m\n\u001b[1;32m   7694\u001b[0m     endpoints: List[InferenceEndpoint] \u001b[38;5;241m=\u001b[39m list_inference_endpoints(namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_namespace(token\u001b[38;5;241m=\u001b[39mtoken))\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/huggingface_hub/hf_api.py:1663\u001b[0m, in \u001b[0;36mHfApi.whoami\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;129m@validate_hf_hub_args\u001b[39m\n\u001b[1;32m   1650\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhoami\u001b[39m(\u001b[38;5;28mself\u001b[39m, token: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m   1651\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;124;03m    Call HF API to know \"whoami\".\u001b[39;00m\n\u001b[1;32m   1653\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;124;03m            To disable authentication, pass `False`.\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m     r \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1662\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/whoami-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m-> 1663\u001b[0m         headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_hf_headers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# If `token` is provided and not `None`, it will be used by default.\u001b[39;49;00m\n\u001b[1;32m   1665\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Otherwise, the token must be retrieved from cache or env variable.\u001b[39;49;00m\n\u001b[1;32m   1666\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1668\u001b[0m     )\n\u001b[1;32m   1669\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1670\u001b[0m         hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/huggingface_hub/hf_api.py:9484\u001b[0m, in \u001b[0;36mHfApi._build_hf_headers\u001b[0;34m(self, token, is_write_action, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   9481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   9482\u001b[0m     \u001b[38;5;66;03m# Cannot do `token = token or self.token` as token can be `False`.\u001b[39;00m\n\u001b[1;32m   9483\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken\n\u001b[0;32m-> 9484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_hf_headers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_write_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_write_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9489\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/huggingface_hub/utils/_headers.py:124\u001b[0m, in \u001b[0;36mbuild_hf_headers\u001b[0;34m(token, is_write_action, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mBuild headers dictionary to send in a HF Hub call.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m        If `token=True` but token is not saved locally.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Get auth token to send\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m token_to_send \u001b[38;5;241m=\u001b[39m \u001b[43mget_token_to_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m _validate_token_to_send(token_to_send, is_write_action\u001b[38;5;241m=\u001b[39mis_write_action)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Combine headers\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/huggingface_hub/utils/_headers.py:158\u001b[0m, in \u001b[0;36mget_token_to_send\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cached_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LocalTokenNotFoundError(\n\u001b[1;32m    159\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken is required (`token=True`), but no token found. You\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m need to provide a token or be logged in to Hugging Face with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `huggingface-cli login` or `huggingface_hub.login`. See\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://huggingface.co/settings/tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         )\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_token\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Case implicit use of the token is forbidden by env variable\u001b[39;00m\n",
      "\u001b[0;31mLocalTokenNotFoundError\u001b[0m: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import traceback\n",
    "import torch\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import langchain\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "langchain.debug = True\n",
    "from huggingface_hub import login\n",
    "login() # You will be prompted for your HF key, which will then be saved locally\n",
    "model_name = \"Qwen/Qwen2.5-3B\"\n",
    "log_file = \"run_log_qwen25.jsonl\"  # 日志\n",
    "ans_file = \"model_logits_qwen25.jsonl\"  # 推理结果\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# llm = HuggingFaceHub(repo_id=model_name, device=device)\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_name,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100},\n",
    "    device=3,\n",
    "    )\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=model_name,\n",
    "#     task=\"text-generation\",\n",
    "#     max_new_tokens=2048,\n",
    "#     do_sample=True,\n",
    "#     huggingfacehub_api_token=\"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\"\n",
    "# )\n",
    "\n",
    "llm = ChatHuggingFace(llm=llm)\n",
    "def generate_response(inp_list, unique_id_list, max_new_tokens=2048):\n",
    "    try:\n",
    "        input_prompts = []\n",
    "        for inp in inp_list:\n",
    "            messages = [\n",
    "                (\"system\",  \"你是一位专业且有帮助的AI助手，专门回答与金融、经济相关的各种问题，包括选择题和开放式问题。对于选择题，请先直接给出正确答案，然后详细说明理由，并逐一分析其他选项的优缺点。你的回答应确保内容严谨、准确，语言应与问题中的主要语言保持一致。\"),\n",
    "                ( \"user\", inp)\n",
    "            ]\n",
    "            template = ChatPromptTemplate.from_messages(messages)\n",
    "            prompt0 = template.format()\n",
    "            print(prompt0)\n",
    "            # prompt = \"\\n\".join(message.content for message in prompt0)\n",
    "            # print(prompt)\n",
    "            \n",
    "            input_prompts.append(prompt0)\n",
    "\n",
    "            responses = []\n",
    "            run_infos = []\n",
    "            for i, prompt in enumerate(input_prompts):\n",
    "                response = llm(prompt)\n",
    "                responses.append(response)\n",
    "                run_info = {\n",
    "                    \"unique_id\": unique_id_list[i],\n",
    "                    \"model_inp\": prompt,\n",
    "                    \"gen_params\": {\n",
    "                        \"max_new_tokens\": max_new_tokens,\n",
    "                        \"top_p\": 0.95,\n",
    "                        \"temperature\": 0.8,\n",
    "                        \"do_sample\": True\n",
    "                    }\n",
    "                }\n",
    "                run_infos.append(run_info)\n",
    "            return responses, run_infos\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return None, ()\n",
    "\n",
    "def generate_unique_code(input_string):\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    sha256_hash.update(input_string.encode('utf-8'))\n",
    "    unique_code = sha256_hash.hexdigest()\n",
    "    return unique_code\n",
    "\n",
    "def load_test_data(test_file):\n",
    "    test_data = []\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data.append(json.loads(line.strip()))\n",
    "    return test_data\n",
    "\n",
    "# 加载测试数据\n",
    "test_data = load_test_data('eval_only_query.jsonl')\n",
    "print(\"test\")\n",
    "\n",
    "batch_size = 16\n",
    "for i in range(0, len(test_data), batch_size):\n",
    "    batch_items = test_data[i:i+batch_size]\n",
    "    prompts = [item['query'] for item in batch_items]\n",
    "    query_types = [item['query_type'] for item in batch_items]\n",
    "    unique_ids = [generate_unique_code(prompt) for prompt in prompts]\n",
    "\n",
    "    responses, run_infos = generate_response(prompts, unique_ids)\n",
    "\n",
    "    if responses is None:\n",
    "        continue\n",
    "\n",
    "    for j in range(len(prompts)):\n",
    "        response = responses[j]\n",
    "        run_info = run_infos[j]\n",
    "        run_info[\"answer\"] = response\n",
    "        # 保存运行日志\n",
    "        with open(log_file, \"a\") as fw:\n",
    "            fw.write(json.dumps(run_info, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # 保存输出结果\n",
    "        answer_info = {\n",
    "            \"query\": prompts[j],\n",
    "            \"query_type\": query_types[j],\n",
    "            \"answer\": response\n",
    "        }\n",
    "\n",
    "        with open(ans_file, \"a\") as fw:\n",
    "            fw.write(json.dumps(answer_info, ensure_ascii=False) + \"\\n\")\n",
    "        print(f\"answer_info: {answer_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
